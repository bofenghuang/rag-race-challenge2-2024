platform: X
topic: DSA-Transparency-Report
subtopic: DSA Transparency Report
file_path: /home/bhuang/nlp/rag-race-challenge2-2024/platform-docs-versions/X_DSA-Transparency-Report/DSA Transparency Report.md
url: https://transparency.twitter.com/dsa-transparency-report.html


## Our Own Initiative Content Moderation Activities

AUTOMATED CONTENT MODERATION

X employs a combination of heuristics and machine learning algorithms to automatically detect content that violates the [X Rules and policies](https://www.google.com/url?q=https://help.twitter.com/en/rules-and-policies/twitter-rules&sa=D&source=editors&ust=1700092924594417&usg=AOvVaw1q35SfdruYaNVu9eLBtY3w) enforced on our platform.

MACHINE LEARNING MODELS

We use combinations of natural language processing models, image processing models and other sophisticated machine learning methods to detect potentially violative content. These models vary in complexity and in the outputs they produce. For example, the model used to detect abuse on the platform is trained on abuse violations detected in the past. Content flagged by these machine learning models are either reviewed by human content reviewers before an action is taken or, in some cases, automatically actioned based on model output.

HEURISTIC MODELS

Heuristics are typically utilised to enable X to react quickly to new forms of violations that emerge on the platform. Heuristics are common patterns of text or keywords that may be typical of a certain category of violations. Pieces of content detected by heuristics may also get reviewed by human content reviewers before an action is taken on the content. These heuristics are used to flag content for review by human agents and prioritise the order such content is reviewed.

TESTING, EVALUATION, AND ITERATION

Automated enforcements under the [X Rules and policies](https://www.google.com/url?q=https://help.twitter.com/en/rules-and-policies/twitter-rules&sa=D&source=editors&ust=1700092924595562&usg=AOvVaw0Y1W5XLAC9s7Em6XFapAsI) undergo rigorous testing before being applied to the live product. Both machine learning and heuristic models are trained and/or validated on thousands of data points and labels (e.g., violative or non-violative) that are generated by trained human content reviewers. For example, inputs to content-related models can include the text within the post itself, the images attached to the post, and other characteristics. Training data for the models comes from both the cases reviewed by our content moderators, random samples, and various other samples of pieces of content from the platform.

Once reviewers have confirmed that the detection meets an acceptable standard of accuracy, we consider the automation to be ready for launch. Once launched, automations are monitored dynamically for ongoing performance and health. If we detect anomalies in performance (for instance, significant spikes or dips against the volume we established during sizing, or significant changes in user complaint/overturn rates), our Engineering (including Data Science) and Policy teams revisit the automation to diagnose any potential problems and adjust the automations as appropriate.

USE OF HUMAN MODERATION

Before any given algorithm is launched to the platform, we verify its detection of policy violating content or behaviour by drawing a statistically significant test sample and performing item-by-item human review. Reviewers have expertise in the applicable policies and are trained by our Policy teams to ensure the reliability of their decisions. During this testing phase, we also calculate the expected volume of moderation actions a given automation is likely to perform in order to set a baseline against which we can monitor for anomalies in the future (called “sizing”). Human review helps us to confirm that these automations achieve a level of precision, and sizing helps us understand what to expect once the automations are launched.

In addition, humans proactively conduct manual content reviews for potential policy violations. We conduct proactive sweeps for certain high-priority categories of potentially violative content both periodically and during major events, such as elections. Agents also proactively review content flagged by heuristic and machine learning models for potential violations of other policies, including our [sensitive media](https://www.google.com/url?q=https://help.twitter.com/en/rules-and-policies/media-policy&sa=D&source=editors&ust=1700092924596795&usg=AOvVaw1mIYtVnccelDc7iAZByplZ), [child sexual exploitation](https://www.google.com/url?q=https://help.twitter.com/en/rules-and-policies/sexual-exploitation-policy&sa=D&source=editors&ust=1700092924597005&usg=AOvVaw1Mvr-HEF6UYJWPUGb5hAjR) (CSE) and [violent and hateful entities](https://www.google.com/url?q=https://help.twitter.com/en/rules-and-policies/violent-entities&sa=D&source=editors&ust=1700092924597231&usg=AOvVaw29QR8JHHTYvlbKfdNma7K6) policies.

AUTOMATED MODERATION ACTIVITY EXAMPLES

A vast majority of all accounts that are suspended for the promotion of terrorism and CSE are proactively flagged by a combination of technology and other purpose-built internal proprietary tools.

When we remove CSE content, we immediately report it to the National Center for Missing and Exploited Children (NCMEC). NCMEC makes reports available to the appropriate law enforcement agencies around the world to facilitate investigations and prosecutions.

Our current methods for surfacing potentially violative terrorist content for review include leveraging the shared industry hash database, e.g., supported by the [Global Internet Forum to Counter Terrorism (GIFCT)](https://www.google.com/url?q=https://twitter.com/GIFCT_official&sa=D&source=editors&ust=1700092924687772&usg=AOvVaw38maBrqkmV70pHrrAkcBDn), and deploying a range of internal tools and/or utilising the industry hash sharing (e.g., [PhotoDNA](https://www.google.com/url?q=https://blog.twitter.com/en_us/topics/company/2021/transparency-19&sa=D&source=editors&ust=1700092924688140&usg=AOvVaw2_ujE1bjwHPe4ePDndg1IQ)) prior to any reports filed. We [commit](https://www.google.com/url?q=https://blog.twitter.com/en_us/topics/company/2019/addressing-the-abuse-of-tech-to-spread-terrorist-and-extremist-c&sa=D&source=editors&ust=1700092924688386&usg=AOvVaw2IgkFIwsqWiVX17JkJXAuX) to continuing to invest in technology that improves our capability to detect and remove, for instance, terrorist and violent extremist content online, including the extension or development of digital fingerprinting and AI-based technology solutions. Our participation in multi-stakeholder communities, such as the [Christchurch Call to Action](https://www.google.com/url?q=https://www.christchurchcall.com/&sa=D&source=editors&ust=1700092924688583&usg=AOvVaw3I4xBW82jgqMmjOfoNqi0k), [Global Internet Forum to Counter Terrorism](https://www.google.com/url?q=https://gifct.org/about/&sa=D&source=editors&ust=1700092924688778&usg=AOvVaw2TbbHJ4qGGO1ZmDuPARc1W) and EU Internet Forum (EUIF), helps to identify emerging trends in how terrorists and violent extremists are using the Internet to promote their content and exploit online platforms.

You can learn more about our commitment to eradicating CSE and terrorist content, and the actions we’ve taken [here](https://www.google.com/url?q=https://transparency.twitter.com/en/reports/rules-enforcement.html%232020-jan-jun&sa=D&source=editors&ust=1700092924689251&usg=AOvVaw3Iq2tuaqEQgjAEp2tofWuQ). Our continued investment in proprietary technology is steadily reducing the burden on people to report this content to us.

SCALED INVESTIGATIONS

These moderation activities are supplemented by scaled human investigations into the tactics, techniques and procedures that bad actors use to circumvent our rules and policies. These investigations may leverage signals and behaviours identifiable on our platform, as well as off-platform information, to identify large-scale and/or technically sophisticated evasions of our detection and enforcement activities. For example, through these investigations, we are able to detect coordinated activity intended to manipulate our platform and artificially amplify the reach of certain accounts or their content.  

CLOSING STATEMENT ON CONTENT MODERATION ACTIVITIES

Our content moderation systems are designed and tailored to mitigate systematic risks without unnecessarily restricting the use of our service and fundamental rights, especially freedom of expression. Content moderation activities are implemented and anchored on principled policies and leverage a diverse set of interventions to ensure that our actions are reasonable, proportionate and effective. Our content moderation systems blend automated and human review paired with a robust appeals system that enables our users to quickly raise potential moderation anomalies or mistakes.