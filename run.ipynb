{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/projects/bhuang/.cache/huggingface\"\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/bhuang/nlp/rag-race-challenge2-2024\")\n",
    "from reader import NotSimpleDirectoryReader, get_file_metadata\n",
    "from category_retriever import ModerateBM25CategoryRetriever\n",
    "from embed_retriever import BGEM3EmbedModel, BGEM3EmbedDocumentRetriever\n",
    "from simple_reader import SimplerReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_csv_file = (\n",
    "    \"/home/bhuang/nlp/rag-race-challenge2-2024/challenge-2-dataset-and-documentation/dataset/train/input/questions.csv\"\n",
    ")\n",
    "\n",
    "# input_dir = \"/home/bhuang/nlp/rag-race-challenge2-2024/platform-docs-versions\"\n",
    "input_dir = \"/home/bhuang/nlp/rag-race-challenge2-2024/platform-docs-versions-sample\"\n",
    "\n",
    "df = pd.read_csv(input_csv_file, sep=\";\")\n",
    "queries = df[\"question\"].tolist()\n",
    "\n",
    "# # load documents\n",
    "\n",
    "documents, documents_chunks, documents_smaller_chunks, documents_smaller_smaller_chunks, smallest_chunks = SimplerReader(\n",
    "    input_dir\n",
    ").load_files()\n",
    "\n",
    "docs = smallest_chunks\n",
    "\n",
    "\"\"\"\n",
    "documents = NotSimpleDirectoryReader(\n",
    "    input_dir=input_dir,\n",
    "    exclude=[\"README.md\"],\n",
    "    recursive=True,\n",
    "    required_exts=[\".md\"],\n",
    "    # file_extractor\n",
    "    file_metadata=get_file_metadata,\n",
    ").load_data()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        # First, try to split along Markdown headings (starting with level 2)\n",
    "        # \"\\n#{1,6} \",\n",
    "        \"\\n#{1,3} \",\n",
    "        # Note the alternative syntax for headings (below) is not handled here\n",
    "        # Heading level 2\n",
    "        # ---------------\n",
    "        # End of code block\n",
    "        # \"```\\n\",\n",
    "        # Horizontal lines\n",
    "        # \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "        # \"\\n---+\\n\",\n",
    "        # \"\\n___+\\n\",\n",
    "        # Note that this splitter doesn't handle horizontal lines defined\n",
    "        # by *three or more* of ***, ---, or ___, but this is not handled\n",
    "        \"\\n\" * 4,\n",
    "        # \"\\n\\n\",\n",
    "        # \"\\n\",\n",
    "        # \" \",\n",
    "        # \"\",\n",
    "    ],\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=0,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "text_splitter = LangchainNodeParser(text_splitter)\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "docs = [\n",
    "    {\n",
    "        \"text\": n.get_content(\"all\"),\n",
    "        \"platform\": n.metadata[\"platform\"],\n",
    "    }\n",
    "    for n in nodes\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "doc_names = list(set([d.metadata[\"platform\"] for d in docs]))\n",
    "\n",
    "# todo: nbest\n",
    "category_document_retriever = ModerateBM25CategoryRetriever(\n",
    "    doc_names,\n",
    "    name_mapping={\"x\": \"x twitter tweet\", \"booking.com\": \"booking\", \"facebook\": \"facebook meta\"},  # todo\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "# Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "embed_model = BGEM3EmbedModel(\"BAAI/bge-m3\", use_fp16=True, device=0)\n",
    "embed_document_retriever = BGEM3EmbedDocumentRetriever(\n",
    "    embed_model,\n",
    "    batch_size=12,\n",
    "    max_query_length=512,\n",
    "    max_document_length=512,\n",
    "    # score_name=\"dense_score\",\n",
    "    # score_name = \"sparse_dense_score\",\n",
    "    score_name=\"colbert_sparse_dense_score\",\n",
    "    # weights_for_different_modes = None,\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "docs = embed_document_retriever.embed_documents(docs)\n",
    "\n",
    "# -- query stage\n",
    "\n",
    "# query = \"How is content moderation carried out on X?\"\n",
    "query = \"How is content moderation carried out on twitter?\"\n",
    "# query = \"I have access to a set of tweets URLs that I consider to be hateful. How can I use Twitter's API to monitor the average duration between the tweet's creation and its moderation?\"\n",
    "\n",
    "docs_, _ = category_document_retriever(query, docs)\n",
    "\n",
    "docs_ = embed_document_retriever(query, docs_)\n",
    "print(docs_)\n",
    "\n",
    "# debug\n",
    "# data = [{\"query\": query, \"catgory\": category_document_retriever(query, docs)[1]} for query in queries]\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_json(\"./tmp_query_category.json\", orient=\"records\", force_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
